{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파인튜닝(Fine-tuning) 가이드\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 개요 및 환경 설정\n",
    "\n",
    "### 1.1 파인튜닝이란?\n",
    "\n",
    "- **파인튜닝(Fine-tuning)** 은 사전 학습된 모델을 특정 도메인이나 작업에 맞게 추가 학습시키는 과정\n",
    "\n",
    "    - 도메인 특화 지식 습득 (예: ETF 투자 전문 지식)\n",
    "    - 응답 스타일 및 포맷 커스터마이징\n",
    "    - 일반 모델보다 높은 정확도\n",
    "    - 비용 효율적 (처음부터 학습하는 것보다)\n",
    "\n",
    "- **LLM vs 임베딩 모델**\n",
    "\n",
    "    | 특징 | LLM 파인튜닝 | 임베딩 모델 파인튜닝 |\n",
    "    |------|-------------|-------------------|\n",
    "    | 목적 | 텍스트 생성, 질의응답 | 의미적 유사도 계산, 검색 |\n",
    "    | 출력 | 자연어 텍스트 | 벡터 임베딩 |\n",
    "    | 주요 사용처 | 챗봇, 요약, 번역 | RAG, 시맨틱 검색, 추천 |\n",
    "    | 데이터 형식 | Q&A 쌍, 대화 | 유사 문장 쌍, triplets |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 환경 설정\n",
    "\n",
    "- **필수 라이브러리 설치**\n",
    "\n",
    "    ```bash\n",
    "    # Unsloth (LLM 파인튜닝용)\n",
    "    pip install unsloth / uv pip install unsloth\n",
    "\n",
    "    # Sentence Transformers (임베딩 모델용)\n",
    "    pip install -U \"sentence-transformers>=3.0\"\n",
    "\n",
    "    # 공통 라이브러리\n",
    "    pip install datasets accelerate torch pandas\n",
    "    pip install python-dotenv  # 환경변수 관리용\n",
    "    ```\n",
    "\n",
    "- **하드웨어 요구사항**\n",
    "\n",
    "- **LLM 파인튜닝 (Unsloth)**\n",
    "    - GPU: NVIDIA GPU 16GB+ VRAM 권장\n",
    "    - 4-bit 양자화로 메모리 4배 절감 가능\n",
    "    - Google Colab (무료 T4), Runpod, Lambda Labs 등 활용 가능\n",
    "\n",
    "- **임베딩 모델 파인튜닝**\n",
    "    - GPU: 8GB+ VRAM\n",
    "    - CPU에서도 가능하지만 느림\n",
    "    - Colab 무료 티어로도 충분\n",
    "\n",
    "- **Hugging Face 토큰 설정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 또는 직접 입력\n",
    "if \"HUGGINGFACE_TOKEN\" not in os.environ:\n",
    "    os.environ[\"HUGGINGFACE_TOKEN\"] = getpass(\"Hugging Face Token: \")\n",
    "    \n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    use_openai = input(\"OpenAI API 사용? (y/n): \")\n",
    "    if use_openai.lower() == 'y':\n",
    "        os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API Key: \")\n",
    "\n",
    "# Hugging Face 로그인\n",
    "from huggingface_hub import login\n",
    "login(token=os.environ[\"HUGGINGFACE_TOKEN\"], add_to_git_credential=True)\n",
    "\n",
    "print(\"✅ 환경 설정 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 데이터 수집 및 전처리\n",
    "\n",
    "- 한국 ETF 시장 데이터를 LLM 파인튜닝에 적합한 형식으로 변환하는 과정을 학습\n",
    "- 다양한 파인튜닝 방법론에 맞는 데이터셋을 구성하는 방법 처리\n",
    "\n",
    "### 2.1 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일에서 로드\n",
    "df = pd.read_csv('./etf_info.csv', encoding='cp949')\n",
    "\n",
    "# 데이터 확인\n",
    "print(f\"데이터 크기: {df.shape}\")\n",
    "print(f\"컬럼: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 컬럼명 영문으로 매핑\n",
    "column_mapping = {\n",
    "    '표준코드': 'standard_code',\n",
    "    '단축코드': 'ticker',\n",
    "    '한글종목명': 'name_kr',\n",
    "    '한글종목약명': 'short_name_kr',\n",
    "    '영문종목명': 'name_en',\n",
    "    '상장일': 'listing_date',\n",
    "    '기초지수명': 'base_index',\n",
    "    '지수산출기관': 'index_provider',\n",
    "    '추적배수': 'tracking_multiplier',\n",
    "    '복제방법': 'replication_method',\n",
    "    '기초시장분류': 'base_market_category',\n",
    "    '기초자산분류': 'base_asset_category',\n",
    "    '상장좌수': 'listed_shares',\n",
    "    '운용사': 'manager',\n",
    "    'CU수량': 'cu_quantity',\n",
    "    '총보수': 'total_expense_ratio',\n",
    "    '과세유형': 'tax_type'\n",
    "}\n",
    "\n",
    "# 데이터프레임의 컬럼명 변경\n",
    "df = df.rename(columns=column_mapping)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train/Test 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 80/20 분할\n",
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    random_state=1207,\n",
    "    stratify=df['tax_type']  # 과세 유형별 균등 분할\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# 저장\n",
    "train_df.to_csv('./etf_train.csv', index=False)\n",
    "test_df.to_csv('./etf_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 기본 데이터셋 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) Alpaca 형식 (LLM용)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def create_alpaca_dataset(df):\n",
    "    \"\"\"기본 Alpaca 형식 데이터셋\"\"\"\n",
    "    alpaca_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # 패턴 1: 기본 정보\n",
    "        alpaca_data.append({\n",
    "            \"instruction\": \"다음 ETF의 기본 정보를 제공해주세요.\",\n",
    "            \"input\": f\"{row['name_kr']} (종목코드: {row['ticker']})\",\n",
    "            \"output\": f\"{row['name_kr']}은 {row['manager']}에서 운용하는 ETF입니다. \"\n",
    "                     f\"기초지수는 {row['base_index']}이며, \"\n",
    "                     f\"총보수는 연 {row['total_expense_ratio']:.2f}%입니다. \"\n",
    "                     f\"{pd.to_datetime(row['listing_date']).strftime('%Y년 %m월')}에 상장되었습니다.\"\n",
    "        })\n",
    "        \n",
    "        # 패턴 2: 투자 특징\n",
    "        alpaca_data.append({\n",
    "            \"instruction\": \"이 ETF의 특징을 알려주세요.\",\n",
    "            \"input\": f\"{row['name_kr']}\",\n",
    "            \"output\": f\"{row['name_kr']}는 {row['base_index']}를 추종하는 ETF로, \"\n",
    "                     f\"{row['replication_method']} 방식으로 운용됩니다. \"\n",
    "                     f\"기초자산은 {row['base_asset_category']}이며, \"\n",
    "                     f\"과세 유형은 {row['tax_type']}입니다.\"\n",
    "        })\n",
    "        \n",
    "        # 패턴 3: 운용사 질문\n",
    "        alpaca_data.append({\n",
    "            \"instruction\": \"이 ETF의 운용사 정보를 알려주세요.\",\n",
    "            \"input\": f\"{row['name_kr']}\",\n",
    "            \"output\": f\"{row['name_kr']}는 {row['manager']}에서 운용하고 있습니다. \"\n",
    "                     f\"총보수는 연 {row['total_expense_ratio']:.2f}%입니다.\"\n",
    "        })\n",
    "        \n",
    "        # 패턴 4: 투자 적합성 (자산 분류별)\n",
    "        suitability = {\n",
    "            '주식': '시장 상승기에 유리하며, 변동성이 높아 리스크 관리가 필요합니다.',\n",
    "            '채권': '안정적인 수익을 추구하며, 금리 변동에 영향을 받습니다.',\n",
    "            '원자재': '인플레이션 헤지 수단으로 활용 가능하며, 시장 변동성이 큽니다.',\n",
    "        }\n",
    "        \n",
    "        advice = suitability.get(\n",
    "            row['base_asset_category'], \n",
    "            '해당 자산군의 특성을 충분히 이해하고 투자하시기 바랍니다.'\n",
    "        )\n",
    "        \n",
    "        alpaca_data.append({\n",
    "            \"instruction\": \"이 ETF는 어떤 투자자에게 적합한가요?\",\n",
    "            \"input\": f\"{row['name_kr']}\",\n",
    "            \"output\": f\"{row['name_kr']}는 {row['base_asset_category']} 자산에 투자하는 ETF입니다. \"\n",
    "                     f\"{advice}\"\n",
    "        })\n",
    "    \n",
    "    return Dataset.from_list(alpaca_data)\n",
    "\n",
    "# 데이터셋 생성\n",
    "train_alpaca = create_alpaca_dataset(train_df)\n",
    "test_alpaca = create_alpaca_dataset(test_df)\n",
    "\n",
    "print(f\"기본 Alpaca 데이터: Train {len(train_alpaca)}, Test {len(test_alpaca)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alpaca[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_alpaca[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 저장\n",
    "output_dir = \"datasets\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "train_alpaca.save_to_disk(f\"{output_dir}/train_alpaca\")\n",
    "test_alpaca.save_to_disk(f\"{output_dir}/test_alpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# 저장된 데이터셋 불러오기\n",
    "train_alpaca = load_from_disk(f\"{output_dir}/train_alpaca\")\n",
    "test_alpaca = load_from_disk(f\"{output_dir}/test_alpaca\")\n",
    "\n",
    "# 데이터셋 확인\n",
    "print(f\"Train: {len(train_alpaca)}\")\n",
    "print(f\"Test: {len(test_alpaca)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Train과 Test를 분리해서 하나의 데이터셋으로 구성\n",
    "llm_alpaca_dataset = DatasetDict({\n",
    "    'train': train_alpaca,\n",
    "    'test': test_alpaca\n",
    "})\n",
    "\n",
    "print(f\"✅ 데이터셋 구성:\")\n",
    "print(f\"   Train: {len(llm_alpaca_dataset['train'])}개\")\n",
    "print(f\"   Test: {len(llm_alpaca_dataset['test'])}개\")\n",
    "\n",
    "# 사용 예시\n",
    "print(\"\\n학습용 샘플:\")\n",
    "print(llm_alpaca_dataset['train'][0])\n",
    "\n",
    "print(\"\\n평가용 샘플:\")\n",
    "print(llm_alpaca_dataset['test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 허깅페이스 업로드\n",
    "llm_alpaca_dataset.push_to_hub(\n",
    "    \"******/etf-alpaca-llm-v1\",  # 자신의 사용자명으로 변경\n",
    "    private=True,  # 비공개 여부 선택\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 허깅페이스 다운로드\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 데이터셋 다운로드\n",
    "train_alpaca = load_dataset(\"******/etf-alpaca-llm-v1\", split=\"train\")\n",
    "test_alpaca = load_dataset(\"******/etf-alpaca-llm-v1\", split=\"test\")\n",
    "\n",
    "# 데이터셋 확인\n",
    "print(f\"Train: {len(train_alpaca)}\")\n",
    "print(f\"Test: {len(test_alpaca)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 임베딩용 Positive Pairs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_dataset(df):\n",
    "    \"\"\"Hard Negatives를 포함한 임베딩 데이터\"\"\"\n",
    "    from itertools import combinations\n",
    "    \n",
    "    pairs = []\n",
    "    \n",
    "    # 1. 기본 Positive Pairs (위와 동일)\n",
    "    for _, row in df.iterrows():\n",
    "        pairs.append({\n",
    "            \"sentence1\": f\"{row['name_kr']}\",\n",
    "            \"sentence2\": f\"{row['base_index']}를 추종하는 {row['manager']} 운용 ETF\",\n",
    "            \"label\": 1  # 유사함\n",
    "        })\n",
    "        \n",
    "        pairs.append({\n",
    "            \"sentence1\": f\"종목코드 {row['ticker']}\",\n",
    "            \"sentence2\": f\"{row['name_kr']} ETF\",\n",
    "            \"label\": 1\n",
    "        })\n",
    "    \n",
    "    # 2. Hard Negatives: 같은 운용사지만 다른 ETF\n",
    "    for manager in df['manager'].unique():\n",
    "        manager_etfs = df[df['manager'] == manager]\n",
    "        \n",
    "        if len(manager_etfs) > 1:\n",
    "            # 같은 운용사의 ETF 조합\n",
    "            for (idx1, row1), (idx2, row2) in combinations(manager_etfs.iterrows(), 2):\n",
    "                pairs.append({\n",
    "                    \"sentence1\": f\"{row1['name_kr']}\",\n",
    "                    \"sentence2\": f\"{row2['name_kr']}\",\n",
    "                    \"label\": 0  # 유사하지 않음 (같은 운용사지만 다른 상품)\n",
    "                })\n",
    "    \n",
    "    # 3. Hard Negatives: 같은 기초지수지만 다른 ETF\n",
    "    for index in df['base_index'].unique():\n",
    "        index_etfs = df[df['base_index'] == index]\n",
    "        \n",
    "        if len(index_etfs) > 1:\n",
    "            for (idx1, row1), (idx2, row2) in combinations(index_etfs.iterrows(), 2):\n",
    "                pairs.append({\n",
    "                    \"sentence1\": f\"종목코드 {row1['ticker']}\",\n",
    "                    \"sentence2\": f\"{row2['name_kr']}\",\n",
    "                    \"label\": 0  # 다른 ETF\n",
    "                })\n",
    "    \n",
    "    # 4. Easy Negatives: 완전히 다른 카테고리\n",
    "    for _, row1 in df.iterrows():\n",
    "        # 다른 자산군 선택\n",
    "        diff_category = df[df['base_asset_category'] != row1['base_asset_category']]\n",
    "        \n",
    "        if len(diff_category) > 0:\n",
    "            row2 = diff_category.sample(1).iloc[0]\n",
    "            pairs.append({\n",
    "                \"sentence1\": f\"{row1['name_kr']}\",\n",
    "                \"sentence2\": f\"{row2['name_kr']}\",\n",
    "                \"label\": 0\n",
    "            })\n",
    "    \n",
    "    return Dataset.from_list(pairs)\n",
    "\n",
    "# 데이터셋 생성\n",
    "train_embedding = create_embedding_dataset(train_df)\n",
    "test_embedding = create_embedding_dataset(test_df)\n",
    "\n",
    "print(f\"임베딩 데이터: Train {len(train_embedding)}, Test {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 저장\n",
    "output_dir = \"datasets\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "train_embedding.save_to_disk(f\"{output_dir}/train_embedding\")\n",
    "test_embedding.save_to_disk(f\"{output_dir}/test_embedding\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 데이터셋 불러오기\n",
    "train_embedding = load_from_disk(f\"{output_dir}/train_embedding\")\n",
    "test_embedding = load_from_disk(f\"{output_dir}/test_embedding\")\n",
    "\n",
    "# 데이터셋 확인\n",
    "print(f\"Train: {len(train_embedding)}\")\n",
    "print(f\"Test: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Train과 Test를 분리해서 하나의 데이터셋으로 구성\n",
    "embedding_dataset = DatasetDict({\n",
    "    'train': train_embedding,\n",
    "    'test': test_embedding\n",
    "})\n",
    "\n",
    "# 데이터셋 허깅페이스 업로드\n",
    "embedding_dataset.push_to_hub(\n",
    "    \"******/etf-embedding-v1\",  # 자신의 사용자명으로 변경\n",
    "    private=True,  # 비공개 여부 선택\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 허깅페이스 다운로드\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 데이터셋 다운로드\n",
    "train_embedding = load_dataset(\"******/etf-embedding-v1\", split=\"train\")\n",
    "test_embedding = load_dataset(\"******/etf-embedding-v1\", split=\"test\")\n",
    "\n",
    "# 데이터셋 확인\n",
    "print(f\"Train: {len(train_embedding)}\")\n",
    "print(f\"Test: {len(test_embedding)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
