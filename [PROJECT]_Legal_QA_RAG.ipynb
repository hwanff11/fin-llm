{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë²•ë¥  ì§ˆì˜ì‘ë‹µ AI ì‹œìŠ¤í…œ (Legal QA with RAG)\n",
    "\n",
    "---\n",
    "\n",
    "## í”„ë¡œì íŠ¸ ê°œìš”\n",
    "\n",
    "**ëª©í‘œ**: ì—¬ëŸ¬ ë²•ë ¹ PDF íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ ë²•ë¥ ì  ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” RAG ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "\n",
    "**ì£¼ìš” ê¸°ëŠ¥**:\n",
    "1. ğŸ“„ ì—¬ëŸ¬ PDF ë²•ë ¹ ë¬¸ì„œ ë¡œë“œ ë° ì²˜ë¦¬\n",
    "2. âœ‚ï¸ ë²•ë¥  ì¡°ë¬¸ì— ìµœì í™”ëœ í…ìŠ¤íŠ¸ ë¶„í• \n",
    "3. ğŸ”¢ ê³ í’ˆì§ˆ ì„ë² ë”© ìƒì„± (í•œêµ­ì–´ ìµœì í™”)\n",
    "4. ğŸ—„ï¸ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ë° ê²€ìƒ‰\n",
    "5. ğŸ¤– ë²•ë¥  ì „ë¬¸ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±\n",
    "6. ğŸ“Š ì¶œì²˜ ë° ê·¼ê±° ì¡°ë¬¸ ëª…ì‹œ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import os\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDF ë¬¸ì„œ ë¡œë“œ\n",
    "\n",
    "ë²•ë ¹ PDF íŒŒì¼ë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
    "DATA_DIR = './data'\n",
    "\n",
    "# PDF íŒŒì¼ ëª©ë¡ í™•ì¸\n",
    "pdf_files = glob(os.path.join(DATA_DIR, '*.pdf'))\n",
    "print(f\"ë°œê²¬ëœ PDF íŒŒì¼ ìˆ˜: {len(pdf_files)}\")\n",
    "print(\"\\níŒŒì¼ ëª©ë¡:\")\n",
    "for pdf_file in pdf_files:\n",
    "    print(f\"  - {os.path.basename(pdf_file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²•ë ¹ PDF íŒŒì¼ë§Œ ì„ íƒ (í•„ìš”ì‹œ ìˆ˜ì •)\n",
    "# ì—¬ê¸°ì„œëŠ” housing_leasing_law.pdfë¥¼ ì‚¬ìš©\n",
    "legal_pdf_files = [\n",
    "    os.path.join(DATA_DIR, 'housing_leasing_law.pdf'),\n",
    "    # ì¶”ê°€ ë²•ë ¹ PDF íŒŒì¼ì´ ìˆë‹¤ë©´ ì—¬ê¸°ì— ì¶”ê°€\n",
    "]\n",
    "\n",
    "# ë¬¸ì„œ ë¡œë“œ\n",
    "all_documents = []\n",
    "\n",
    "for pdf_file in legal_pdf_files:\n",
    "    if os.path.exists(pdf_file):\n",
    "        print(f\"\\në¡œë”© ì¤‘: {os.path.basename(pdf_file)}\")\n",
    "        loader = PyPDFLoader(pdf_file)\n",
    "        documents = loader.load()\n",
    "        all_documents.extend(documents)\n",
    "        print(f\"  âœ“ {len(documents)} í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ\")\n",
    "    else:\n",
    "        print(f\"  âœ— íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_file}\")\n",
    "\n",
    "print(f\"\\nì´ ë¡œë“œëœ í˜ì´ì§€ ìˆ˜: {len(all_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²« ë²ˆì§¸ ë¬¸ì„œ í™•ì¸\n",
    "if all_documents:\n",
    "    print(\"ì²« ë²ˆì§¸ í˜ì´ì§€ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "    print(\"=\" * 100)\n",
    "    print(all_documents[0].page_content[:500])\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"\\në©”íƒ€ë°ì´í„°: {all_documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. í…ìŠ¤íŠ¸ ë¶„í•  (ë²•ë¥  ì¡°ë¬¸ ìµœì í™”)\n",
    "\n",
    "ë²•ë¥  ë¬¸ì„œëŠ” ì¡°ë¬¸ ë‹¨ìœ„ë¡œ êµ¬ì¡°í™”ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, ì´ë¥¼ ê³ ë ¤í•œ ë¶„í•  ì „ëµì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ë²•ë¥  ë¬¸ì„œì— ìµœì í™”ëœ í…ìŠ¤íŠ¸ ë¶„í• ê¸°\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,              # ë²•ë¥  ì¡°ë¬¸ì„ í¬í•¨í•  ìˆ˜ ìˆëŠ” ì ì ˆí•œ í¬ê¸°\n",
    "    chunk_overlap=200,           # ë¬¸ë§¥ ë³´ì¡´ì„ ìœ„í•œ ì¤‘ë³µ\n",
    "    length_function=len,\n",
    "    separators=[\n",
    "        \"\\n\\n\",                   # ë‹¨ë½ êµ¬ë¶„\n",
    "        \"\\n\",                     # ì¤„ êµ¬ë¶„\n",
    "        \"ì œ\",                     # ì¡°ë¬¸ êµ¬ë¶„ (ì˜ˆ: ì œ1ì¡°, ì œ2ì¡°)\n",
    "        \"ã€‚\",                     # ë¬¸ì¥ êµ¬ë¶„\n",
    "        \". \",                    # ë¬¸ì¥ êµ¬ë¶„\n",
    "        \" \",                     # ë‹¨ì–´ êµ¬ë¶„\n",
    "        \"\",\n",
    "    ],\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# ë¬¸ì„œ ë¶„í• \n",
    "split_documents = text_splitter.split_documents(all_documents)\n",
    "\n",
    "print(f\"ë¶„í• ëœ ì²­í¬ ìˆ˜: {len(split_documents)}\")\n",
    "print(f\"í‰ê·  ì²­í¬ í¬ê¸°: {sum(len(doc.page_content) for doc in split_documents) / len(split_documents):.0f} ë¬¸ì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶„í• ëœ ì²­í¬ ìƒ˜í”Œ í™•ì¸\n",
    "print(\"ë¶„í• ëœ ì²­í¬ ìƒ˜í”Œ (ì²˜ìŒ 3ê°œ):\")\n",
    "print(\"=\" * 100)\n",
    "for i, doc in enumerate(split_documents[:3], 1):\n",
    "    print(f\"\\n[ì²­í¬ {i}]\")\n",
    "    print(f\"ê¸¸ì´: {len(doc.page_content)} ë¬¸ì\")\n",
    "    print(f\"ë‚´ìš©: {doc.page_content[:300]}...\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ì„ë² ë”© ëª¨ë¸ ì„¤ì •\n",
    "\n",
    "í•œêµ­ì–´ ë²•ë¥  ë¬¸ì„œì— ìµœì í™”ëœ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# OpenAI ì„ë² ë”© ëª¨ë¸ (í•œêµ­ì–´ ì„±ëŠ¥ ìš°ìˆ˜)\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # ë¹„ìš© íš¨ìœ¨ì ì´ë©´ì„œ ì„±ëŠ¥ ì¢‹ìŒ\n",
    "    # model=\"text-embedding-3-large\",  # ë” ë†’ì€ ì„±ëŠ¥ì´ í•„ìš”í•œ ê²½ìš°\n",
    ")\n",
    "\n",
    "print(\"âœ“ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "print(f\"  ëª¨ë¸: {embeddings.model}\")\n",
    "print(f\"  ì°¨ì›: {embeddings.dimensions if hasattr(embeddings, 'dimensions') and embeddings.dimensions else '1536 (ê¸°ë³¸ê°’)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•\n",
    "\n",
    "Chroma DBë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œ ë””ë ‰í† ë¦¬\n",
    "VECTOR_DB_DIR = \"./chroma_db_legal\"\n",
    "\n",
    "# Chroma ë²¡í„° ì €ì¥ì†Œ ìƒì„±\n",
    "print(\"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘...\")\n",
    "print(\"(ì„ë² ë”© ìƒì„±ì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\\n\")\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=split_documents,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"legal_documents\",\n",
    "    persist_directory=VECTOR_DB_DIR,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"},  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ìš©\n",
    ")\n",
    "\n",
    "print(f\"âœ“ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"  ì €ì¥ëœ ë¬¸ì„œ ìˆ˜: {len(vector_store.get()['ids'])}\")\n",
    "print(f\"  ì €ì¥ ìœ„ì¹˜: {VECTOR_DB_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ê²€ìƒ‰ê¸°(Retriever) ì„¤ì •\n",
    "\n",
    "ë‹¤ì–‘í•œ ê²€ìƒ‰ ì „ëµì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ìœ ì‚¬ë„ ê²€ìƒ‰\n",
    "retriever_similarity = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}  # ìƒìœ„ 5ê°œ ë¬¸ì„œ ê²€ìƒ‰\n",
    ")\n",
    "\n",
    "# MMR ê²€ìƒ‰ (ë‹¤ì–‘ì„± ê³ ë ¤)\n",
    "retriever_mmr = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 5,\n",
    "        \"fetch_k\": 20,\n",
    "        \"lambda_mult\": 0.7,  # 0.7 = ê´€ë ¨ì„±ê³¼ ë‹¤ì–‘ì„±ì˜ ê· í˜•\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"âœ“ ê²€ìƒ‰ê¸° ì„¤ì • ì™„ë£Œ\")\n",
    "print(\"  - ìœ ì‚¬ë„ ê²€ìƒ‰ (retriever_similarity)\")\n",
    "print(\"  - MMR ê²€ìƒ‰ (retriever_mmr)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "test_query = \"ì„ëŒ€ì°¨ ê³„ì•½ ê¸°ê°„ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\"\n",
    "\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ì§ˆë¬¸: {test_query}\\n\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "retrieved_docs = retriever_mmr.invoke(test_query)\n",
    "\n",
    "print(f\"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}\\n\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"[ë¬¸ì„œ {i}]\")\n",
    "    print(f\"ì¶œì²˜: {doc.metadata.get('source', 'Unknown')} (í˜ì´ì§€ {doc.metadata.get('page', 'N/A')})\")\n",
    "    print(f\"ë‚´ìš©: {doc.page_content[:200]}...\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ë²•ë¥  ì „ë¬¸ í”„ë¡¬í”„íŠ¸ ì„¤ê³„\n",
    "\n",
    "ë²•ë¥  ì§ˆì˜ì‘ë‹µì— íŠ¹í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ë²•ë¥  ì „ë¬¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
    "system_prompt = \"\"\"ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë²•ë¥  ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "<ì—­í•  ë° ì±…ì„>\n",
    "- ì œê³µëœ ë²•ë ¹ ì¡°ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë²•ë¥  ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "- ë²•ë¥  ìš©ì–´ë¥¼ ì¼ë°˜ì¸ë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
    "- ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ë²•ë ¹ ì¡°ë¬¸ì„ ë°˜ë“œì‹œ ëª…ì‹œí•©ë‹ˆë‹¤.\n",
    "\n",
    "<ë‹µë³€ ì›ì¹™>\n",
    "1. **ì •í™•ì„±**: ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.\n",
    "2. **ëª…í™•ì„±**: ë²•ë¥  ìš©ì–´ë¥¼ ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
    "3. **ê·¼ê±° ì œì‹œ**: ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ì¡°ë¬¸ì„ ëª…í™•íˆ ë°í™ë‹ˆë‹¤.\n",
    "4. **í•œê³„ ì¸ì •**: ì»¨í…ìŠ¤íŠ¸ì— ì •ë³´ê°€ ì—†ìœ¼ë©´ \"ì œê³µëœ ë²•ë ¹ì—ì„œ í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"ë¼ê³  ë‹µë³€í•©ë‹ˆë‹¤.\n",
    "5. **ì£¼ì˜ì‚¬í•­**: ë²•ë¥  ìë¬¸ì´ ì•„ë‹˜ì„ ëª…ì‹œí•˜ê³ , êµ¬ì²´ì ì¸ ì‚¬ì•ˆì€ ì „ë¬¸ê°€ ìƒë‹´ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "<ë‹µë³€ í˜•ì‹>\n",
    "1. **í•µì‹¬ ë‹µë³€**: ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€\n",
    "2. **ë²•ì  ê·¼ê±°**: ê´€ë ¨ ë²•ë ¹ ì¡°ë¬¸ ì¸ìš© (ì¡°ë¬¸ ë²ˆí˜¸ í¬í•¨)\n",
    "3. **ì¶”ê°€ ì„¤ëª…**: í•„ìš”ì‹œ ë³´ì¶© ì„¤ëª…\n",
    "4. **ì£¼ì˜ì‚¬í•­**: ë²•ë¥  ìë¬¸ì´ ì•„ë‹˜ì„ ëª…ì‹œ\n",
    "\n",
    "ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"<ì œê³µëœ ë²•ë ¹ ì¡°ë¬¸>\n",
    "{context}\n",
    "</ì œê³µëœ ë²•ë ¹ ì¡°ë¬¸>\n",
    "\n",
    "<ì§ˆë¬¸>\n",
    "{question}\n",
    "</ì§ˆë¬¸>\n",
    "\n",
    "ìœ„ ë²•ë ¹ ì¡°ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
    "\"\"\"\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", user_prompt)\n",
    "])\n",
    "\n",
    "print(\"âœ“ ë²•ë¥  ì „ë¬¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RAG ì²´ì¸ êµ¬ì„±\n",
    "\n",
    "ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM ì„¤ì •\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # ë˜ëŠ” \"gpt-4o\" (ë” ë†’ì€ ì„±ëŠ¥)\n",
    "    temperature=0.1,       # ë‚®ì€ temperatureë¡œ ì¼ê´€ì„± ìˆëŠ” ë‹µë³€\n",
    ")\n",
    "\n",
    "# ë¬¸ì„œ í¬ë§·íŒ… í•¨ìˆ˜\n",
    "def format_docs(docs):\n",
    "    \"\"\"ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í”„ë¡¬í”„íŠ¸ì— ë§ê²Œ í¬ë§·íŒ…\"\"\"\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        page = doc.metadata.get('page', 'N/A')\n",
    "        formatted.append(\n",
    "            f\"[ë¬¸ì„œ {i}] (ì¶œì²˜: {os.path.basename(source)}, í˜ì´ì§€: {page})\\n\"\n",
    "            f\"{doc.page_content}\\n\"\n",
    "        )\n",
    "    return \"\\n---\\n\".join(formatted)\n",
    "\n",
    "# RAG ì²´ì¸ ìƒì„±\n",
    "rag_chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": retriever_mmr | format_docs,\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"âœ“ RAG ì²´ì¸ êµ¬ì„± ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ë²•ë¥  ì§ˆì˜ì‘ë‹µ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ì‹¤ì œ ë²•ë¥  ì§ˆë¬¸ìœ¼ë¡œ ì‹œìŠ¤í…œì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤\n",
    "test_questions = [\n",
    "    \"ì£¼íƒ ì„ëŒ€ì°¨ ê³„ì•½ì˜ ìµœì†Œ ê¸°ê°„ì€ ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "    \"ì „ì„¸ê¸ˆ ë°˜í™˜ì€ ì–¸ì œ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?\",\n",
    "    \"ì„ëŒ€ì¸ì´ ê³„ì•½ ê°±ì‹ ì„ ê±°ë¶€í•  ìˆ˜ ìˆëŠ” ê²½ìš°ëŠ” ì–¸ì œì¸ê°€ìš”?\",\n",
    "    \"ì„ì°¨ì¸ì˜ ìš°ì„ ë³€ì œê¶Œì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "]\n",
    "\n",
    "print(\"ë²•ë¥  ì§ˆì˜ì‘ë‹µ í…ŒìŠ¤íŠ¸\\n\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n[ì§ˆë¬¸ {i}] {question}\\n\")\n",
    "    \n",
    "    # RAG ì²´ì¸ ì‹¤í–‰\n",
    "    answer = rag_chain.invoke(question)\n",
    "    \n",
    "    print(f\"[ë‹µë³€]\\n{answer}\")\n",
    "    print(\"\\n\" + \"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤\n",
    "\n",
    "ì‚¬ìš©ìê°€ ì§ì ‘ ì§ˆë¬¸í•  ìˆ˜ ìˆëŠ” ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_legal_question(question: str) -> str:\n",
    "    \"\"\"ë²•ë¥  ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    print(f\"\\nì§ˆë¬¸: {question}\\n\")\n",
    "    print(\"ë‹µë³€ ìƒì„± ì¤‘...\\n\")\n",
    "    \n",
    "    answer = rag_chain.invoke(question)\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(answer)\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "# ask_legal_question(\"ì„ëŒ€ì°¨ ê³„ì•½ ê¸°ê°„ ì¤‘ ì„ëŒ€ë£Œë¥¼ ì¸ìƒí•  ìˆ˜ ìˆë‚˜ìš”?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”\n",
    "my_question = \"ì„ëŒ€ì°¨ ê³„ì•½ ê¸°ê°„ ì¤‘ ì„ëŒ€ë£Œë¥¼ ì¸ìƒí•  ìˆ˜ ìˆë‚˜ìš”?\"\n",
    "\n",
    "answer = ask_legal_question(my_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„\n",
    "\n",
    "ì–´ë–¤ ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_retrieval(question: str):\n",
    "    \"\"\"ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìƒì„¸íˆ ë¶„ì„\"\"\"\n",
    "    print(f\"ì§ˆë¬¸: {question}\\n\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # ê²€ìƒ‰ ì‹¤í–‰\n",
    "    retrieved_docs = retriever_mmr.invoke(question)\n",
    "    \n",
    "    print(f\"\\nê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}\\n\")\n",
    "    \n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        print(f\"\\n[ë¬¸ì„œ {i}]\")\n",
    "        print(f\"ì¶œì²˜: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        print(f\"í˜ì´ì§€: {doc.metadata.get('page', 'N/A')}\")\n",
    "        print(f\"ë¬¸ì„œ ê¸¸ì´: {len(doc.page_content)} ë¬¸ì\")\n",
    "        print(f\"\\në‚´ìš©:\\n{doc.page_content}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "# analyze_retrieval(\"ì „ì„¸ê¸ˆ ë°˜í™˜ì€ ì–¸ì œ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. ì¶”ê°€ PDF íŒŒì¼ ì¶”ê°€í•˜ê¸°\n",
    "\n",
    "ìƒˆë¡œìš´ ë²•ë ¹ PDFë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_legal_documents(pdf_paths: list):\n",
    "    \"\"\"\n",
    "    ìƒˆë¡œìš´ ë²•ë ¹ PDF íŒŒì¼ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì¶”ê°€\n",
    "    \n",
    "    Args:\n",
    "        pdf_paths: ì¶”ê°€í•  PDF íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    print(\"ìƒˆë¡œìš´ ë¬¸ì„œ ì¶”ê°€ ì¤‘...\\n\")\n",
    "    \n",
    "    # ë¬¸ì„œ ë¡œë“œ\n",
    "    new_documents = []\n",
    "    for pdf_path in pdf_paths:\n",
    "        if os.path.exists(pdf_path):\n",
    "            print(f\"ë¡œë”©: {os.path.basename(pdf_path)}\")\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            docs = loader.load()\n",
    "            new_documents.extend(docs)\n",
    "            print(f\"  âœ“ {len(docs)} í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ\")\n",
    "        else:\n",
    "            print(f\"  âœ— íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}\")\n",
    "    \n",
    "    if not new_documents:\n",
    "        print(\"\\nì¶”ê°€í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ë¶„í• \n",
    "    print(f\"\\ní…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...\")\n",
    "    split_docs = text_splitter.split_documents(new_documents)\n",
    "    print(f\"  âœ“ {len(split_docs)} ì²­í¬ ìƒì„± ì™„ë£Œ\")\n",
    "    \n",
    "    # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì¶”ê°€\n",
    "    print(f\"\\në²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì¶”ê°€ ì¤‘...\")\n",
    "    vector_store.add_documents(split_docs)\n",
    "    print(f\"  âœ“ ì¶”ê°€ ì™„ë£Œ\")\n",
    "    \n",
    "    print(f\"\\ní˜„ì¬ ì´ ë¬¸ì„œ ìˆ˜: {len(vector_store.get()['ids'])}\")\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ:\n",
    "# new_pdfs = [\n",
    "#     './data/ìƒˆë¡œìš´_ë²•ë ¹1.pdf',\n",
    "#     './data/ìƒˆë¡œìš´_ë²•ë ¹2.pdf',\n",
    "# ]\n",
    "# add_new_legal_documents(new_pdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. ì‹œìŠ¤í…œ ì„±ëŠ¥ ê°œì„  íŒ\n",
    "\n",
    "### ë” ë‚˜ì€ ì„±ëŠ¥ì„ ìœ„í•œ ë°©ë²•:\n",
    "\n",
    "1. **ì„ë² ë”© ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ**\n",
    "   ```python\n",
    "   # text-embedding-3-large ì‚¬ìš© (ë” ë†’ì€ ì •í™•ë„)\n",
    "   embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "   ```\n",
    "\n",
    "2. **LLM ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ**\n",
    "   ```python\n",
    "   # GPT-4o ì‚¬ìš© (ë” ì •í™•í•œ ë²•ë¥  í•´ì„)\n",
    "   llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.1)\n",
    "   ```\n",
    "\n",
    "3. **ì²­í¬ í¬ê¸° ì¡°ì •**\n",
    "   - ë²•ë¥  ì¡°ë¬¸ì´ ê¸¸ë©´ `chunk_size`ë¥¼ ëŠ˜ë¦¬ê¸°\n",
    "   - ë” ì„¸ë°€í•œ ê²€ìƒ‰ì´ í•„ìš”í•˜ë©´ `chunk_size`ë¥¼ ì¤„ì´ê¸°\n",
    "\n",
    "4. **ê²€ìƒ‰ ê°œìˆ˜ ì¡°ì •**\n",
    "   ```python\n",
    "   # ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ê°€ í•„ìš”í•œ ê²½ìš°\n",
    "   retriever = vector_store.as_retriever(\n",
    "       search_type=\"mmr\",\n",
    "       search_kwargs={\"k\": 10}  # 10ê°œ ë¬¸ì„œ ê²€ìƒ‰\n",
    "   )\n",
    "   ```\n",
    "\n",
    "5. **ì¬ìˆœìœ„í™”(Reranking) ì¶”ê°€**\n",
    "   - ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¬ì •ë ¬í•˜ì—¬ ë” ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ìš°ì„ \n",
    "   - Cohere Rerank API ë˜ëŠ” Cross-Encoder ì‚¬ìš©\n",
    "\n",
    "6. **ì¿¼ë¦¬ í™•ì¥(Query Expansion)**\n",
    "   - ì‚¬ìš©ì ì§ˆë¬¸ì„ ì—¬ëŸ¬ í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ê²€ìƒ‰\n",
    "   - Multi-Query Retriever ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. í”„ë¡œì íŠ¸ ì €ì¥ ë° ì¬ì‚¬ìš©\n",
    "\n",
    "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ì´ë¯¸ `./chroma_db_legal`ì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ë‹¤ìŒë²ˆì— ì‚¬ìš©í•  ë•Œ:\n",
    "\n",
    "```python\n",
    "# ê¸°ì¡´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ\n",
    "vector_store = Chroma(\n",
    "    persist_directory=\"./chroma_db_legal\",\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"legal_documents\"\n",
    ")\n",
    "\n",
    "# ê²€ìƒ‰ê¸° ìƒì„±\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n",
    "# RAG ì²´ì¸ ì¬êµ¬ì„±\n",
    "# (ìœ„ì˜ ì½”ë“œ ì¬ì‚¬ìš©)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. ì£¼ì˜ì‚¬í•­ ë° ë©´ì±…ì¡°í•­\n",
    "\n",
    "âš ï¸ **ì¤‘ìš”í•œ ì£¼ì˜ì‚¬í•­**:\n",
    "\n",
    "1. **ë²•ë¥  ìë¬¸ ì•„ë‹˜**: ì´ ì‹œìŠ¤í…œì€ ë²•ë¥  ì •ë³´ ì œê³µ ëª©ì ì´ë©°, ê³µì‹ì ì¸ ë²•ë¥  ìë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤.\n",
    "\n",
    "2. **ì „ë¬¸ê°€ ìƒë‹´ í•„ìš”**: êµ¬ì²´ì ì¸ ë²•ë¥  ë¬¸ì œëŠ” ë°˜ë“œì‹œ ë³€í˜¸ì‚¬ ë“± ë²•ë¥  ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”.\n",
    "\n",
    "3. **ìµœì‹ ì„± í™•ì¸**: ë²•ë ¹ì€ ìˆ˜ì‹œë¡œ ê°œì •ë˜ë¯€ë¡œ, ìµœì‹  ë²•ë ¹ì„ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "4. **ì •í™•ì„± ê²€ì¦**: AIì˜ ë‹µë³€ì€ ì°¸ê³ ìš©ì´ë©°, ì¤‘ìš”í•œ ê²°ì • ì „ì—ëŠ” ì›ë¬¸ì„ ì§ì ‘ í™•ì¸í•˜ì„¸ìš”.\n",
    "\n",
    "5. **ê°œì¸ì •ë³´ ë³´í˜¸**: ì‹¤ì œ ì‚¬ê±´ì´ë‚˜ ê°œì¸ì •ë³´ë¥¼ ì…ë ¥í•˜ì§€ ë§ˆì„¸ìš”."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
