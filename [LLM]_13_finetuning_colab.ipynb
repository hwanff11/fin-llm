{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou2oxL-OgtYS"
      },
      "source": [
        "# LLM ë° ì„ë² ë”© ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹¤ìŠµ (Colab)\n",
        "\n",
        "1. **LLM íŒŒì¸íŠœë‹** (Unsloth + Llama-3.1)\n",
        "   - ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ íŠ¹ì • ë„ë©”ì¸(ETF ì •ë³´)ì— ë§ê²Œ í•™ìŠµ\n",
        "   - LoRA ê¸°ë²•ì„ ì‚¬ìš©í•œ íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹\n",
        "\n",
        "2. **ì„ë² ë”© ëª¨ë¸ íŒŒì¸íŠœë‹** (Sentence Transformers)\n",
        "   - í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë¸ í•™ìŠµ\n",
        "   - ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë¬¸ì¥ì„ ê°€ê¹ê²Œ ë°°ì¹˜"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T07u4g4kgtYV"
      },
      "source": [
        "---\n",
        "\n",
        "## 1. í™˜ê²½ ì„¤ì •\n",
        "\n",
        "- **GPU (Graphics Processing Unit)**:\n",
        "    - ì›ë˜ ê·¸ë˜í”½ ì²˜ë¦¬ìš©ìœ¼ë¡œ ì„¤ê³„ë¨\n",
        "    - ìˆ˜ì²œ ê°œì˜ ì‘ì€ ì½”ì–´ê°€ ë³‘ë ¬ë¡œ ì‘ë™\n",
        "    - ë”¥ëŸ¬ë‹ì˜ í–‰ë ¬ ì—°ì‚°ì— ìµœì í™”ë¨\n",
        "    - CPU ëŒ€ë¹„ 10~100ë°° ë¹ ë¥¸ í•™ìŠµ ì†ë„\n",
        "\n",
        "- **CUDA**:\n",
        "    - NVIDIA GPUë¥¼ í”„ë¡œê·¸ë˜ë°í•˜ê¸° ìœ„í•œ í”Œë«í¼\n",
        "    - PyTorch, TensorFlow ë“±ì´ CUDAë¥¼ ì‚¬ìš©\n",
        "    - GPU ë©”ëª¨ë¦¬(VRAM)ê°€ í´ìˆ˜ë¡ í° ëª¨ë¸ í•™ìŠµ ê°€ëŠ¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb76XCQ1gtYV"
      },
      "outputs": [],
      "source": [
        "# GPU í™•ì¸\n",
        "# í˜„ì¬ ì‹œìŠ¤í…œì— ì‚¬ìš© ê°€ëŠ¥í•œ GPU ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "# - GPU ëª¨ë¸ëª…, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, ì˜¨ë„ ë“±ì„ í‘œì‹œ\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8p6C96ogtYW"
      },
      "source": [
        "### í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "\n",
        "1. **unsloth**: LLM íŒŒì¸íŠœë‹ì„ ìµœì í™”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ (ìµœëŒ€ 80%)\n",
        "   - í•™ìŠµ ì†ë„ í–¥ìƒ (ìµœëŒ€ 2ë°°)\n",
        "   - LoRA, QLoRA ë“± íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹ ì§€ì›\n",
        "\n",
        "2. **sentence-transformers**: ì„ë² ë”© ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "   - í…ìŠ¤íŠ¸ë¥¼ ê³ ì • ê¸¸ì´ ë²¡í„°ë¡œ ë³€í™˜\n",
        "   - ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°ì— ì‚¬ìš©\n",
        "\n",
        "3. **pandas**: ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„\n",
        "\n",
        "4. **python-dotenv**: í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬ (.env íŒŒì¼)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjCVj-qtgtYW"
      },
      "outputs": [],
      "source": [
        "# Colab í™˜ê²½ì—ì„œ Unsloth ë° í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "%%capture\n",
        "import os, re\n",
        "import torch\n",
        "\n",
        "# Unsloth ì„¤ì¹˜ (Colab ì „ìš©)\n",
        "v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "!pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2\n",
        "\n",
        "# ê¸°íƒ€ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "!pip install pandas python-dotenv ipykernel\n",
        "!pip install -U \"sentence-transformers>=3.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYejNoJhgtYW"
      },
      "source": [
        "### Hugging Face ì¸ì¦\n",
        "\n",
        "- **Hugging Face**: ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ê³µìœ í•˜ëŠ” í”Œë«í¼\n",
        "    - **Hub**: ëª¨ë¸, ë°ì´í„°ì…‹, ë°ëª¨ë¥¼ í˜¸ìŠ¤íŒ…\n",
        "    - **Token**: API ì ‘ê·¼ì„ ìœ„í•œ ì¸ì¦ í‚¤\n",
        "    - **ê³µê°œ vs ë¹„ê³µê°œ**: ëª¨ë¸ì„ ê³µê°œí•˜ê±°ë‚˜ ê°œì¸ìš©ìœ¼ë¡œ ì„¤ì • ê°€ëŠ¥\n",
        "\n",
        "- **Token ìƒì„± ë°©ë²•**:\n",
        "    1. [Hugging Face](https://huggingface.co) ì ‘ì†\n",
        "    2. Settings â†’ Access Tokens\n",
        "    3. New Token â†’ Write ê¶Œí•œ ì„ íƒ\n",
        "    4. ìƒì„±ëœ í† í° ë³µì‚¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAW4K9SigtYX"
      },
      "outputs": [],
      "source": [
        "# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Colab Secretsì—ì„œ í† í° ë¡œë“œ ì‹œë„\n",
        "try:\n",
        "    os.environ[\"HUGGINGFACE_TOKEN\"] = userdata.get('HUGGINGFACE_TOKEN')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# ì§ì ‘ ì…ë ¥ (Secretsì— ì—†ëŠ” ê²½ìš°)\n",
        "if \"HUGGINGFACE_TOKEN\" not in os.environ or not os.environ[\"HUGGINGFACE_TOKEN\"]:\n",
        "    from getpass import getpass\n",
        "    # getpass: ì…ë ¥ ì‹œ í™”ë©´ì— í† í°ì´ í‘œì‹œë˜ì§€ ì•ŠìŒ (ë³´ì•ˆ)\n",
        "    os.environ[\"HUGGINGFACE_TOKEN\"] = getpass(\"Hugging Face Token: \")\n",
        "\n",
        "# Hugging Faceì— ë¡œê·¸ì¸\n",
        "# add_to_git_credential=True: git ìê²©ì¦ëª…ì— ì¶”ê°€í•˜ì—¬ ë‹¤ìŒì— ìë™ ë¡œê·¸ì¸\n",
        "login(token=os.environ[\"HUGGINGFACE_TOKEN\"], add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9cNwyxFgtYX"
      },
      "source": [
        "### ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í™˜ê²½ í™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlULrOH7gtYX"
      },
      "outputs": [],
      "source": [
        "# ê¸°ë³¸ import\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset, DatasetDict\n",
        "import torch\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°\n",
        "\n",
        "# PyTorch ë° CUDA ì •ë³´ ì¶œë ¥\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    # Compute Capability: GPUì˜ ê¸°ëŠ¥ ìˆ˜ì¤€ (ë†’ì„ìˆ˜ë¡ ìµœì‹  ê¸°ëŠ¥ ì§€ì›)\n",
        "    print(f\"CUDA Capability: {torch.cuda.get_device_capability(0)}\")\n",
        "    # VRAM: GPU ë©”ëª¨ë¦¬ (ëª¨ë¸ í¬ê¸°ì™€ ë°°ì¹˜ í¬ê¸°ë¥¼ ê²°ì •í•˜ëŠ” ì¤‘ìš”í•œ ìš”ì†Œ)\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERUhsBR6gtYX"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. ë°ì´í„° ì¤€ë¹„\n",
        "\n",
        "- **LLM íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ (Alpaca í˜•ì‹)**:\n",
        "  ```json\n",
        "  {\n",
        "    \"instruction\": \"ì´ ETFì˜ ê¸°ë³¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.\",\n",
        "    \"input\": \"TIGER 200\",\n",
        "    \"output\": \"TIGER 200ì€ KOSPI 200 ì§€ìˆ˜ë¥¼ ì¶”ì¢…í•˜ëŠ”...\"\n",
        "  }\n",
        "  ```\n",
        "\n",
        "- **ì„ë² ë”© íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ (Sentence Pair)**:\n",
        "  ```json\n",
        "  {\n",
        "    \"sentence1\": \"TIGER 200ì€ KOSPI 200ì„ ì¶”ì¢…í•©ë‹ˆë‹¤\",\n",
        "    \"sentence2\": \"ì´ ETFëŠ” í•œêµ­ ì£¼ìš” 200ê°œ ê¸°ì—…ì— íˆ¬ìí•©ë‹ˆë‹¤\"\n",
        "  }\n",
        "  ```\n",
        "  - ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë¬¸ì¥ ìŒì„ í•™ìŠµ\n",
        "  - ëª¨ë¸ì´ ê°™ì€ ì˜ë¯¸ë¥¼ ê°€ì§„ ë¬¸ì¥ì„ ê°€ê¹ê²Œ ì„ë² ë”©í•˜ë„ë¡ í•™ìŠµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvJ-Js18gtYY"
      },
      "outputs": [],
      "source": [
        "# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì • (Colabì˜ ì„ì‹œ ìŠ¤í† ë¦¬ì§€)\n",
        "WORKSPACE = \"/content\"\n",
        "DATA_DIR = f\"{WORKSPACE}/etf_data\"\n",
        "MODEL_DIR = f\"{WORKSPACE}/models\"\n",
        "\n",
        "import os\n",
        "# exist_ok=True: ë””ë ‰í† ë¦¬ê°€ ì´ë¯¸ ì¡´ì¬í•´ë„ ì—ëŸ¬ ë°œìƒí•˜ì§€ ì•ŠìŒ\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"ì‘ì—… ë””ë ‰í† ë¦¬: {WORKSPACE}\")\n",
        "print(f\"ë°ì´í„°: {DATA_DIR}\")\n",
        "print(f\"ëª¨ë¸: {MODEL_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETquK0BUgtYY"
      },
      "source": [
        "### Hugging Face ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "\n",
        "- **Train Set (í•™ìŠµ ë°ì´í„°)**:\n",
        "    - ëª¨ë¸ì´ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ë° ì‚¬ìš©\n",
        "    - ì¼ë°˜ì ìœ¼ë¡œ ì „ì²´ ë°ì´í„°ì˜ 80-90%\n",
        "\n",
        "- **Test Set (í…ŒìŠ¤íŠ¸ ë°ì´í„°)**:\n",
        "    - ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í‰ê°€\n",
        "    - í•™ìŠµì— ì‚¬ìš©ë˜ì§€ ì•Šì€ ìƒˆë¡œìš´ ë°ì´í„°\n",
        "    - ê³¼ì í•©(overfitting) ì—¬ë¶€ í™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7s3lWrOngtYY"
      },
      "outputs": [],
      "source": [
        "# HF ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "from datasets import load_dataset\n",
        "\n",
        "# LLM ë°ì´í„°ì…‹\n",
        "username = '********'   # ê°ì ë³¸ì¸ì˜ Huggingface IDë¡œ ì„¤ì •\n",
        "\n",
        "# ì˜ˆì‹œ ë°ì´í„°ì…‹ì´ ì—†ë‹¤ë©´ redwiggler/alpaca-cleaned ë“±ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "# alpaca_dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
        "alpaca_dataset = load_dataset(f\"{username}/etf-alpaca-llm-v1\")\n",
        "\n",
        "print(f\"âœ… LLM ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ!\")\n",
        "print(f\"   Train: {len(alpaca_dataset['train'])}ê°œ\")\n",
        "print(f\"   Test: {len(alpaca_dataset['test'])}ê°œ\")\n",
        "\n",
        "# ì„ë² ë”© ë°ì´í„°ì…‹\n",
        "embedding_dataset = load_dataset(f\"{username}/etf-embedding-v1\")\n",
        "\n",
        "print(f\"âœ… ì„ë² ë”© ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ!\")\n",
        "print(f\"   Train: {len(embedding_dataset['train'])}ê°œ\")\n",
        "print(f\"   Test: {len(embedding_dataset['test'])}ê°œ\")\n",
        "\n",
        "# ìƒ˜í”Œ í™•ì¸\n",
        "print(\"\\nLLM ìƒ˜í”Œ:\")\n",
        "print(alpaca_dataset['train'][0])\n",
        "\n",
        "print(\"\\nì„ë² ë”© ìƒ˜í”Œ:\")\n",
        "print(embedding_dataset['train'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IDlWI8TgtYY"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. LLM íŒŒì¸íŠœë‹\n",
        "\n",
        "- **ì‚¬ì „ í•™ìŠµ(Pre-training)**:\n",
        "    - ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ ëª¨ë¸ì„ í•™ìŠµ\n",
        "    - ì¼ë°˜ì ì¸ ì–¸ì–´ íŒ¨í„´ê³¼ ì§€ì‹ ìŠµë“\n",
        "    - ì˜ˆ: GPT, LlamaëŠ” ì¸í„°ë„·ì˜ ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ë¡œ í•™ìŠµë¨\n",
        "\n",
        "- **íŒŒì¸íŠœë‹(Fine-tuning)**:\n",
        "    - ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ íŠ¹ì • ì‘ì—…/ë„ë©”ì¸ì— ë§ê²Œ ì¶”ê°€ í•™ìŠµ\n",
        "    - ì ì€ ë°ì´í„°ë¡œë„ íš¨ê³¼ì \n",
        "    - ì˜ˆ: ETF ì •ë³´ì— íŠ¹í™”ëœ ëª¨ë¸ ë§Œë“¤ê¸°\n",
        "\n",
        "- **LoRA** (Low-Rank Adaptation)\n",
        "\n",
        "    - **ì „í†µì  íŒŒì¸íŠœë‹ì˜ ë¬¸ì œ**:\n",
        "        - ëª¨ë“  íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ â†’ ë©”ëª¨ë¦¬ ë§ì´ í•„ìš”\n",
        "        - í•™ìŠµ ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼\n",
        "        - ëª¨ë¸ í¬ê¸°ë§Œí¼ì˜ ì €ì¥ ê³µê°„ í•„ìš”\n",
        "\n",
        "    - **LoRAì˜ í•´ê²°ì±…**:\n",
        "        - ì›ë³¸ ëª¨ë¸ì€ ê³ ì •(freeze)\n",
        "        - ì‘ì€ ì–´ëŒ‘í„°(adapter) ë ˆì´ì–´ë§Œ í•™ìŠµ\n",
        "        - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 80% ê°ì†Œ\n",
        "        - í•™ìŠµ ì†ë„ 2-3ë°° í–¥ìƒ\n",
        "        - ì–´ëŒ‘í„°ë§Œ ì €ì¥ (ìˆ˜ì‹­ MB vs ìˆ˜ì‹­ GB)\n",
        "\n",
        "    - **LoRA íŒŒë¼ë¯¸í„°**:\n",
        "        - `r`: rank (ë‚®ì„ìˆ˜ë¡ íŒŒë¼ë¯¸í„° ì ìŒ, ì¼ë°˜ì ìœ¼ë¡œ 8-64)\n",
        "        - `lora_alpha`: í•™ìŠµë¥  ìŠ¤ì¼€ì¼ë§\n",
        "        - `target_modules`: ì–´ë–¤ ë ˆì´ì–´ì— LoRA ì ìš©í• ì§€\n",
        "\n",
        "- **Quantization (ì–‘ìí™”)**\n",
        "    - ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë‚®ì€ ì •ë°€ë„ë¡œ ì €ì¥\n",
        "    - FP32 (32ë¹„íŠ¸) â†’ INT4/INT8 (4ë¹„íŠ¸/8ë¹„íŠ¸)\n",
        "    - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 1/4 ~ 1/8ë¡œ ê°ì†Œ\n",
        "    - ì„±ëŠ¥ ì†ì‹¤ ìµœì†Œ (1-2%)\n",
        "    - ë” í° ëª¨ë¸ì„ ê°™ì€ GPUì—ì„œ í•™ìŠµ ê°€ëŠ¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94vKsnkvgtYY"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# ëª¨ë¸ ì„ íƒ\n",
        "MODEL_NAME = \"unsloth/Meta-Llama-3.1-8B\"  # Llama 3.1 8B ëª¨ë¸\n",
        "MAX_SEQ_LENGTH = 4096  # ìµœëŒ€ í† í° ê¸¸ì´ (Colab T4ì—ì„œëŠ” ë©”ëª¨ë¦¬ ì£¼ì˜)\n",
        "\n",
        "# ëª¨ë¸ ë¡œë“œ\n",
        "# load_in_4bit: 4ë¹„íŠ¸ ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€ (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
        "# load_in_8bit: 8ë¹„íŠ¸ ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€ (4ë¹„íŠ¸ë³´ë‹¤ ì •í™•, ë©”ëª¨ë¦¬ 2ë°°)\n",
        "# full_finetuning: ì „ì²´ íŒŒë¼ë¯¸í„° í•™ìŠµ vs LoRA (False ê¶Œì¥)\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = MODEL_NAME,\n",
        "    max_seq_length = MAX_SEQ_LENGTH,\n",
        "    load_in_4bit = True,  # Colab T4ì—ì„œëŠ” 4ë¹„íŠ¸ í•„ìˆ˜\n",
        "    load_in_8bit = False,\n",
        "    full_finetuning = False,  # LoRA ì‚¬ìš©\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv05m46SgtYY"
      },
      "source": [
        "### í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë° ë°ì´í„° ì „ì²˜ë¦¬\n",
        "\n",
        "- **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**:\n",
        "    - ëª¨ë¸ì—ê²Œ ì‘ì—…ì„ ëª…í™•í•˜ê²Œ ì§€ì‹œí•˜ëŠ” ë°©ë²•\n",
        "    - ì¼ê´€ëœ í˜•ì‹ìœ¼ë¡œ ì…ë ¥ì„ êµ¬ì¡°í™”\n",
        "\n",
        "- **Alpaca í˜•ì‹**:\n",
        "\n",
        "    ```\n",
        "    ### Instruction:\n",
        "    [ë¬´ì—‡ì„ í•´ì•¼ í•˜ëŠ”ì§€]\n",
        "\n",
        "    ### Input:\n",
        "    [êµ¬ì²´ì ì¸ ì…ë ¥ ë°ì´í„°]\n",
        "\n",
        "    ### Response:\n",
        "    [ëª¨ë¸ì˜ ì¶œë ¥]\n",
        "    ```\n",
        "\n",
        "- **EOS Token (End of Sequence)**:\n",
        "    - ë¬¸ì¥/ëŒ€í™”ì˜ ëì„ í‘œì‹œí•˜ëŠ” íŠ¹ìˆ˜ í† í°\n",
        "    - ì¶”ê°€í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë¸ì´ ê³„ì† ìƒì„±í•¨\n",
        "    - í•™ìŠµ ì‹œ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EshWCgOzgtYY"
      },
      "outputs": [],
      "source": [
        "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input. Write a response.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token  # ë¬¸ì¥ ì¢…ë£Œ í† í° (í•„ìˆ˜!)\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"\n",
        "    ë°ì´í„°ë¥¼ í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜\n",
        "\n",
        "    Args:\n",
        "        examples: instruction, input, outputì„ í¬í•¨í•œ ë°°ì¹˜ ë°ì´í„°\n",
        "\n",
        "    Returns:\n",
        "        í”„ë¡¬í”„íŠ¸ í˜•ì‹ì˜ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸\n",
        "    \"\"\"\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # EOS_TOKENì„ ë°˜ë“œì‹œ ì¶”ê°€í•´ì•¼ ìƒì„±ì´ ë¬´í•œíˆ ê³„ì†ë˜ì§€ ì•ŠìŒ\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "\n",
        "    return { \"text\" : texts }\n",
        "\n",
        "# ë°ì´í„°ì…‹ ì „ì²˜ë¦¬\n",
        "# batched=True: ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ì—¬ ì†ë„ í–¥ìƒ\n",
        "train_data = alpaca_dataset['train'].map(formatting_prompts_func, batched=True)\n",
        "test_data  = alpaca_dataset['test'].map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# ë³€í™˜ëœ ë°ì´í„° í™•ì¸\n",
        "print(train_data[0]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HdJoUJogtYY"
      },
      "source": [
        "### í•™ìŠµ ì „ ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
        "\n",
        "- **Inference(ì¶”ë¡ ) ëª¨ë“œ**:\n",
        "    - í•™ìŠµì´ ì•„ë‹Œ ì˜ˆì¸¡/ìƒì„± ëª¨ë“œ\n",
        "    - ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
        "    - ë“œë¡­ì•„ì›ƒ ë“± í•™ìŠµìš© ê¸°ëŠ¥ ë¹„í™œì„±í™”\n",
        "\n",
        "- **ìƒì„± íŒŒë¼ë¯¸í„°**:\n",
        "    - `max_new_tokens`: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜\n",
        "    - `temperature`: ë‹¤ì–‘ì„± ì¡°ì ˆ (ë†’ì„ìˆ˜ë¡ ì°½ì˜ì , ë‚®ì„ìˆ˜ë¡ ê²°ì •ì )\n",
        "    - `top_p`: nucleus sampling (ìƒìœ„ p% í™•ë¥  í† í°ë§Œ ê³ ë ¤)\n",
        "    - `repetition_penalty`: ë°˜ë³µ ë°©ì§€ (>1.0ì´ë©´ ë°˜ë³µ ì–µì œ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP5yIdcvgtYZ"
      },
      "outputs": [],
      "source": [
        "# ëª¨ë¸ì„ ì¶”ë¡  ëª¨ë“œë¡œ ì „í™˜\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def test_model(instruction, input_text=\"\"):\n",
        "    \"\"\"\n",
        "    ëª¨ë¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜\n",
        "\n",
        "    Args:\n",
        "        instruction: ì‘ì—… ì§€ì‹œ\n",
        "        input_text: ì…ë ¥ ë°ì´í„°\n",
        "\n",
        "    Returns:\n",
        "        ëª¨ë¸ì˜ ì‘ë‹µ\n",
        "    \"\"\"\n",
        "    # í”„ë¡¬í”„íŠ¸ ìƒì„± (Response ë¶€ë¶„ì€ ë¹„ì›Œë‘ )\n",
        "    prompt = alpaca_prompt.format(instruction, input_text, \"\")\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,       # ìµœëŒ€ 512 í† í° ìƒì„±\n",
        "        temperature=0.7,          # ì ë‹¹í•œ ë‹¤ì–‘ì„±\n",
        "        top_p=0.9,                # nucleus sampling\n",
        "        repetition_penalty=1.1    # ë°˜ë³µ ì–µì œ\n",
        "    )\n",
        "\n",
        "    # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Response ë¶€ë¶„ë§Œ ì¶”ì¶œ\n",
        "    return response.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "# ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\n",
        "test_cases = [\n",
        "    (\"ë‹¤ìŒ ETFì˜ ê¸°ë³¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.\", \"ACE ë ˆë²„ë¦¬ì§€\"),\n",
        "    (\"ì´ ETFì˜ íˆ¬ì ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\", \"TIGER KOFR\"),\n",
        "    (\"ì–´ë–¤ ETFë¥¼ ì¶”ì²œí•˜ë‚˜ìš”?\", \"ì•ˆì •ì ì¸ íˆ¬ìë¥¼ ì›í•©ë‹ˆë‹¤\")\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"í•™ìŠµ ì „ ëª¨ë¸ í…ŒìŠ¤íŠ¸\")\n",
        "print(\"=\"*60)\n",
        "for inst, inp in test_cases:\n",
        "    print(f\"\\n[ì§ˆë¬¸] {inst}\")\n",
        "    print(f\"[ì…ë ¥] {inp}\")\n",
        "    print(f\"[ë‹µë³€] {test_model(inst, inp)}\")\n",
        "    print(\"-\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRAyFGj_gtYZ"
      },
      "source": [
        "### LoRA ì–´ëŒ‘í„° ì¶”ê°€\n",
        "\n",
        "- **r (rank)**:\n",
        "    - LoRA í–‰ë ¬ì˜ ì°¨ì›\n",
        "    - ë‚®ì„ìˆ˜ë¡ íŒŒë¼ë¯¸í„° ìˆ˜ ê°ì†Œ\n",
        "    - ê¶Œì¥ê°’: 8 (ê°„ë‹¨), 16-32 (ì¼ë°˜), 64-128 (ë³µì¡)\n",
        "\n",
        "- **target_modules**:\n",
        "    - LoRAë¥¼ ì ìš©í•  ë ˆì´ì–´ ì„ íƒ\n",
        "    - `q_proj, k_proj, v_proj`: Attentionì˜ Query, Key, Value\n",
        "    - `o_proj`: Attention ì¶œë ¥\n",
        "    - `gate_proj, up_proj, down_proj`: Feed-Forward ë„¤íŠ¸ì›Œí¬\n",
        "\n",
        "- **lora_alpha**:\n",
        "    - LoRA ì—…ë°ì´íŠ¸ì˜ ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜\n",
        "    - ì¼ë°˜ì ìœ¼ë¡œ rê³¼ ê°™ì€ ê°’ ì‚¬ìš©\n",
        "\n",
        "- **lora_dropout**:\n",
        "    - ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ë“œë¡­ì•„ì›ƒ\n",
        "    - 0 = ìµœì í™”ë¨ (Unsloth ê¶Œì¥)\n",
        "\n",
        "- **use_gradient_checkpointing**:\n",
        "    - ë©”ëª¨ë¦¬ ì ˆì•½ ê¸°ë²•\n",
        "    - \"unsloth\" = Unsloth ìµœì í™” ë²„ì „ (30% ë©”ëª¨ë¦¬ ì ˆì•½)\n",
        "    - ë°°ì¹˜ í¬ê¸°ë¥¼ 2ë°° ëŠ˜ë¦´ ìˆ˜ ìˆìŒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPT-MB9wgtYZ"
      },
      "outputs": [],
      "source": [
        "# í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜\n",
        "FastLanguageModel.for_training(model)\n",
        "\n",
        "# LoRA ì–´ëŒ‘í„° ì¶”ê°€\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 8,  # LoRA rank (íŒŒë¼ë¯¸í„° ìˆ˜ ê²°ì •)\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention ë ˆì´ì–´\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",     # FFN ë ˆì´ì–´\n",
        "    ],\n",
        "    lora_alpha = 8,  # ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜\n",
        "    lora_dropout = 0,  # ë“œë¡­ì•„ì›ƒ (0 = ìµœì í™”)\n",
        "    bias = \"none\",     # ë°”ì´ì–´ìŠ¤ í•™ìŠµ ì•ˆ í•¨ (ìµœì í™”)\n",
        "    use_gradient_checkpointing = \"unsloth\",  # ë©”ëª¨ë¦¬ ì ˆì•½ (30%)\n",
        "    random_state = 3407,  # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ\n",
        "    use_rslora = False,   # Rank-Stabilized LoRA\n",
        "    loftq_config = None,  # LoftQ ì–‘ìí™”\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORPgWMN4gtYZ"
      },
      "source": [
        "### í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì • ë° ì‹œì‘\n",
        "\n",
        "- **ë°°ì¹˜(Batch)**:\n",
        "    - í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ìƒ˜í”Œ ìˆ˜\n",
        "    - `per_device_train_batch_size`: GPUë‹¹ ë°°ì¹˜ í¬ê¸°\n",
        "    - í´ìˆ˜ë¡ í•™ìŠµ ì•ˆì •ì , ë©”ëª¨ë¦¬ ë§ì´ í•„ìš”\n",
        "\n",
        "- **Gradient Accumulation**:\n",
        "    - ì—¬ëŸ¬ ë¯¸ë‹ˆë°°ì¹˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ëˆ„ì \n",
        "    - ì‹¤ì œ ë°°ì¹˜ í¬ê¸° = batch_size Ã— accumulation_steps\n",
        "    - ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ìœ ìš©\n",
        "\n",
        "- **ì—í¬í¬(Epoch)**:\n",
        "    - ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆ í•™ìŠµí•˜ëŠ” ë‹¨ìœ„\n",
        "    - `num_train_epochs`: ì´ ì—í¬í¬ ìˆ˜\n",
        "    - ë˜ëŠ” `max_steps`: ì´ ìŠ¤í… ìˆ˜ë¡œ ì œí•œ ê°€ëŠ¥\n",
        "\n",
        "- **í•™ìŠµë¥ (Learning Rate)**:\n",
        "    - íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ í¬ê¸°\n",
        "    - ë„ˆë¬´ í¬ë©´ ë¶ˆì•ˆì •, ë„ˆë¬´ ì‘ìœ¼ë©´ ëŠë¦¼\n",
        "    - LoRA: 2e-4 ~ 5e-4 ê¶Œì¥\n",
        "\n",
        "- **Warmup**:\n",
        "    - ì´ˆë°˜ì— í•™ìŠµë¥ ì„ ì„œì„œíˆ ì¦ê°€\n",
        "    - í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ\n",
        "\n",
        "- **Learning Rate Scheduler**:\n",
        "    - `cosine`: ì½”ì‚¬ì¸ í•¨ìˆ˜ í˜•íƒœë¡œ ê°ì†Œ (ì¼ë°˜ì )\n",
        "    - `linear`: ì„ í˜•ìœ¼ë¡œ ê°ì†Œ\n",
        "    - `constant`: ê³ ì •\n",
        "\n",
        "- **Mixed Precision (FP16/BF16)**:\n",
        "    - FP32 ëŒ€ì‹  FP16/BF16 ì‚¬ìš©\n",
        "    - ë©”ëª¨ë¦¬ ì ˆë°˜, ì†ë„ 2ë°°\n",
        "    - BF16ì´ ë” ì•ˆì •ì  (Ampere GPU ì´ìƒ)\n",
        "\n",
        "- **ì˜µí‹°ë§ˆì´ì €**:\n",
        "    - `adamw_8bit`: 8ë¹„íŠ¸ AdamW (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
        "    - `adamw_torch`: í‘œì¤€ AdamW\n",
        "    - `sgd`: Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nID_qOagtYZ"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# í•™ìŠµ ì„¤ì •\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    dataset_text_field=\"text\",  # í…ìŠ¤íŠ¸ê°€ ì €ì¥ëœ í•„ë“œëª…\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    args=TrainingArguments(\n",
        "        # ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)\n",
        "        per_device_train_batch_size=2,   # Colab T4: 2 ê¶Œì¥\n",
        "        gradient_accumulation_steps=4,   # ì‹¤ì œ ë°°ì¹˜ = 2 Ã— 4 = 8\n",
        "\n",
        "        # í•™ìŠµ ê¸°ê°„\n",
        "        warmup_steps=10,        # ì´ˆë°˜ ì›Œë°ì—… ìŠ¤í…\n",
        "        # num_train_epochs=2,     # ì´ 2 ì—í¬í¬ í•™ìŠµ\n",
        "        max_steps=60,         # ë˜ëŠ” ìŠ¤í… ìˆ˜ë¡œ ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)\n",
        "\n",
        "        # ìµœì í™”\n",
        "        learning_rate=2e-4,            # í•™ìŠµë¥ \n",
        "        optim=\"adamw_8bit\",            # 8ë¹„íŠ¸ AdamW\n",
        "        weight_decay=0.01,             # ê°€ì¤‘ì¹˜ ê°ì‡  (ì •ê·œí™”)\n",
        "        lr_scheduler_type=\"cosine\",    # ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ëŸ¬\n",
        "\n",
        "        # Mixed Precision\n",
        "        fp16=not torch.cuda.is_bf16_supported(),  # BF16 ì§€ì› ì•ˆ ë˜ë©´ FP16\n",
        "        bf16=torch.cuda.is_bf16_supported(),      # BF16 ì§€ì›ë˜ë©´ ì‚¬ìš©\n",
        "\n",
        "        # ë¡œê¹… ë° ì €ì¥\n",
        "        logging_steps=1,              # ë§¤ ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸\n",
        "        save_strategy=\"epoch\",        # ì—í¬í¬ë§ˆë‹¤ ì €ì¥\n",
        "        eval_strategy=\"epoch\",        # ì—í¬í¬ë§ˆë‹¤ í‰ê°€\n",
        "        load_best_model_at_end=True,  # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ\n",
        "\n",
        "        # ì¶œë ¥ ë””ë ‰í† ë¦¬\n",
        "        output_dir=f\"{MODEL_DIR}/llm_outputs\",\n",
        "        seed=3407,           # ì¬í˜„ì„±\n",
        "        report_to=\"none\",    # wandb ë“± ë¹„í™œì„±í™”\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"ğŸš€ í•™ìŠµ ì‹œì‘...\")\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# í•™ìŠµ ì‹¤í–‰\n",
        "trainer.train()\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"âœ… í•™ìŠµ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {elapsed/60:.2f}ë¶„)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXj87784gtYZ"
      },
      "source": [
        "### í•™ìŠµ í›„ ëª¨ë¸ í…ŒìŠ¤íŠ¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzYAjJungtYZ"
      },
      "outputs": [],
      "source": [
        "# ëª¨ë¸ì„ ì¶”ë¡  ëª¨ë“œë¡œ ì „í™˜\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def test_model(instruction, input_text=\"\"):\n",
        "    prompt = alpaca_prompt.format(instruction, input_text, \"\")\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "# ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\n",
        "test_cases = [\n",
        "    (\"ë‹¤ìŒ ETFì˜ ê¸°ë³¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.\", \"ACE ë ˆë²„ë¦¬ì§€\"),\n",
        "    (\"ì´ ETFì˜ íˆ¬ì ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\", \"TIGER KOFR\"),\n",
        "    (\"ì–´ë–¤ ETFë¥¼ ì¶”ì²œí•˜ë‚˜ìš”?\", \"ì•ˆì •ì ì¸ íˆ¬ìë¥¼ ì›í•©ë‹ˆë‹¤\")\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"í•™ìŠµ í›„ ëª¨ë¸ í…ŒìŠ¤íŠ¸\")\n",
        "print(\"=\"*60)\n",
        "for inst, inp in test_cases:\n",
        "    print(f\"\\n[ì§ˆë¬¸] {inst}\")\n",
        "    print(f\"[ì…ë ¥] {inp}\")\n",
        "    print(f\"[ë‹µë³€] {test_model(inst, inp)}\")\n",
        "    print(\"-\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4Ps1iWYgtYZ"
      },
      "source": [
        "### ëª¨ë¸ ì €ì¥\n",
        "\n",
        "- **1. LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥**:\n",
        "    - ê°€ì¥ ì‘ì€ í¬ê¸° (ìˆ˜ì‹­ MB)\n",
        "    - ì›ë³¸ ëª¨ë¸ê³¼ í•¨ê»˜ ë¡œë“œ í•„ìš”\n",
        "    - Hugging Faceì— ì—…ë¡œë“œ ê¶Œì¥\n",
        "\n",
        "- **2. ë³‘í•©ëœ 16-bit ëª¨ë¸**:\n",
        "    - ì›ë³¸ + LoRAë¥¼ í•˜ë‚˜ë¡œ ë³‘í•©\n",
        "    - ë…ë¦½ì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥\n",
        "    - í¬ê¸° ì¦ê°€ (ìˆ˜ GB)\n",
        "\n",
        "- **3. GGUF í˜•ì‹**:\n",
        "    - llama.cpp, Ollama ë“±ì—ì„œ ì‚¬ìš©\n",
        "    - ë‹¤ì–‘í•œ ì–‘ìí™” ìˆ˜ì¤€ ì§€ì›\n",
        "    - `q4_k_m`: 4ë¹„íŠ¸ (ë¹ ë¥´ê³  ì‘ìŒ)\n",
        "    - `q8_0`: 8ë¹„íŠ¸ (ë” ì •í™•)\n",
        "    - CPUì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7s6tDIXgtYZ"
      },
      "outputs": [],
      "source": [
        "# ëª¨ë¸ ì €ì¥ (ì—¬ëŸ¬ í˜•ì‹)\n",
        "save_dir = f\"{MODEL_DIR}/etf_llama_model\"\n",
        "\n",
        "# 1. LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥ (ê°€ì¥ ì‘ìŒ, ê¶Œì¥)\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "print(f\"âœ… LoRA ì–´ëŒ‘í„° ì €ì¥: {save_dir}\")\n",
        "\n",
        "# 2. ë³‘í•©ëœ 16-bit ëª¨ë¸ ì €ì¥ (ë°°í¬ìš©)\n",
        "# model.save_pretrained_merged(\n",
        "#     f\"{save_dir}_merged_16bit\",\n",
        "#     tokenizer,\n",
        "#     save_method=\"merged_16bit\"\n",
        "# )\n",
        "# print(f\"âœ… ë³‘í•© ëª¨ë¸ ì €ì¥: {save_dir}_merged_16bit\")\n",
        "\n",
        "# 3. GGUF í˜•ì‹ ì €ì¥ (Ollama, llama.cppìš©)\n",
        "# model.save_pretrained_gguf(\n",
        "#     f\"{save_dir}_gguf\",\n",
        "#     tokenizer,\n",
        "#     quantization_method=[\"q4_k_m\", \"q8_0\"]  # 4ë¹„íŠ¸, 8ë¹„íŠ¸ ë²„ì „\n",
        "# )\n",
        "# print(f\"âœ… GGUF ì €ì¥: {save_dir}_gguf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN8QzHH9gtYa"
      },
      "source": [
        "### Hugging Face Hubì— ì—…ë¡œë“œ\n",
        "\n",
        "- **Hugging Face Hubì˜ ì¥ì **:\n",
        "    - í´ë¼ìš°ë“œ ì €ì¥ì†Œ (ë¬´ë£Œ)\n",
        "    - ë²„ì „ ê´€ë¦¬ (Git ê¸°ë°˜)\n",
        "    - ì–´ë””ì„œë“  ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥\n",
        "    - íŒ€ í˜‘ì—… ê°€ëŠ¥\n",
        "    - ê³µê°œ/ë¹„ê³µê°œ ì„¤ì •\n",
        "\n",
        "- **ë¦¬í¬ì§€í† ë¦¬ ëª…ëª… ê·œì¹™**:\n",
        "    ```\n",
        "    username/model-name\n",
        "    ì˜ˆ: john/llama-7B-etf-finetuned\n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onOdQJPZgtYa"
      },
      "outputs": [],
      "source": [
        "# Hugging Face ì—…ë¡œë“œ\n",
        "repo_name = f\"{username}/llama-7B-etf-finetuned\"\n",
        "\n",
        "# LoRA ì–´ëŒ‘í„° ì—…ë¡œë“œ\n",
        "model.push_to_hub(\n",
        "    repo_name,\n",
        "    token=os.environ[\"HUGGINGFACE_TOKEN\"],\n",
        "    private=True  # ë¹„ê³µê°œ ë¦¬í¬ì§€í† ë¦¬\n",
        ")\n",
        "tokenizer.push_to_hub(\n",
        "    repo_name,\n",
        "    token=os.environ[\"HUGGINGFACE_TOKEN\"],\n",
        "    private=True\n",
        ")\n",
        "print(f\"âœ… ëª¨ë¸ ì—…ë¡œë“œ ì™„ë£Œ: https://huggingface.co/{repo_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRlyXjm2gtYa"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. ì„ë² ë”©(Embedding) ëª¨ë¸ íŒŒì¸íŠœë‹\n",
        "\n",
        "- **ì„ë² ë”©ì˜ ì •ì˜**:\n",
        "    - í…ìŠ¤íŠ¸ë¥¼ ê³ ì • ê¸¸ì´ì˜ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜\n",
        "    - ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ â†’ ê°€ê¹Œìš´ ë²¡í„°\n",
        "    - ì˜ˆ: \"ê°•ì•„ì§€\"ì™€ \"ê°œ\" â†’ ìœ ì‚¬í•œ ë²¡í„°\n",
        "\n",
        "- **ì„ë² ë”©ì˜ í™œìš©**:\n",
        "    1. **ìœ ì‚¬ë„ ê²€ìƒ‰**: ê°€ì¥ ê´€ë ¨ ìˆëŠ” ë¬¸ì„œ ì°¾ê¸°\n",
        "    2. **í´ëŸ¬ìŠ¤í„°ë§**: ë¹„ìŠ·í•œ í…ìŠ¤íŠ¸ ê·¸ë£¹í™”\n",
        "    3. **ë¶„ë¥˜**: í…ìŠ¤íŠ¸ ì¹´í…Œê³ ë¦¬ ì˜ˆì¸¡\n",
        "    4. **RAG**: ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ LLMì— ì œê³µ\n",
        "\n",
        "- **Sentence-BERT (SBERT)**:\n",
        "    - BERTë¥¼ ë¬¸ì¥ ì„ë² ë”©ì— ìµœì í™”\n",
        "    - Siamese ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°\n",
        "    - ë¹ ë¥¸ ì†ë„ (BERT ëŒ€ë¹„ 100ë°°)\n",
        "\n",
        "- **BGE-M3 ëª¨ë¸**:\n",
        "    - **B**AIRU **G**eneral **E**mbedding\n",
        "    - **M**ulti-lingual (ë‹¤êµ­ì–´ ì§€ì›)\n",
        "    - **M**ulti-granularity (ë‹¤ì–‘í•œ ê¸¸ì´)\n",
        "    - **M**ulti-functionality (ë‹¤ì–‘í•œ íƒœìŠ¤í¬)\n",
        "    - 1024ì°¨ì› ì„ë² ë”©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Euy88cqXgtYa"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ëª¨ë¸ ë¡œë“œ\n",
        "# BGE-M3: ë‹¤êµ­ì–´ ì§€ì›, ê³ ì„±ëŠ¥ ì„ë² ë”© ëª¨ë¸\n",
        "emb_model = SentenceTransformer(\"BAAI/bge-m3\")\n",
        "print(f\"ì„ë² ë”© ì°¨ì›: {emb_model.get_sentence_embedding_dimension()}\")\n",
        "print(f\"ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {emb_model.max_seq_length}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGVRJkISgtYa"
      },
      "source": [
        "### ì†ì‹¤ í•¨ìˆ˜ ë° í‰ê°€ì ì„¤ì •\n",
        "\n",
        "- **Multiple Negatives Ranking Loss**: ëŒ€ì¡° í•™ìŠµ(Contrastive Learning)\n",
        "    - Positive Pair: ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë¬¸ì¥ ìŒ\n",
        "    - Negative Samples: ë°°ì¹˜ ë‚´ ë‹¤ë¥¸ ë¬¸ì¥ë“¤\n",
        "    - ëª©í‘œ: PositiveëŠ” ê°€ê¹ê²Œ, NegativeëŠ” ë©€ê²Œ\n",
        "    - ë°°ì¹˜ í¬ê¸°ê°€ í´ìˆ˜ë¡ íš¨ê³¼ì  (ë” ë§ì€ negative samples)\n",
        "\n",
        "- **Matryoshka Embeddings**: ë§ˆíŠ¸ë£Œì‹œì¹´ ì¸í˜• ì›ë¦¬\n",
        "    - í•˜ë‚˜ì˜ ëª¨ë¸ì´ ì—¬ëŸ¬ ì°¨ì›ì˜ ì„ë² ë”© ìƒì„±\n",
        "    - ì˜ˆ: 768ì°¨ì›, 512ì°¨ì›, 256ì°¨ì›, 128ì°¨ì›, 64ì°¨ì›\n",
        "    - ì‚¬ìš© ì‹œ í•„ìš”í•œ ì°¨ì›ë§Œ ì„ íƒ\n",
        "    - ì €ì°¨ì›: ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ ì ˆì•½\n",
        "    - ê³ ì°¨ì›: ë” ì •í™•\n",
        "\n",
        "- **ì¥ì **:\n",
        "    - ìœ ì—°ì„±: ìƒí™©ì— ë§ê²Œ ì°¨ì› ì„ íƒ\n",
        "    - íš¨ìœ¨ì„±: í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ë‹¤ì–‘í•œ ìš©ë„\n",
        "    - ì„±ëŠ¥ ì €í•˜ ìµœì†Œí™”\n",
        "\n",
        "- **Spearman ìƒê´€ê³„ìˆ˜**: ì„ë² ë”© í‰ê°€\n",
        "    - ìˆœìœ„ ê¸°ë°˜ ìƒê´€ê´€ê³„ ì¸¡ì •\n",
        "    - -1 ~ 1 ë²”ìœ„\n",
        "    - 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ\n",
        "    - ì¸ê°„ì˜ ìœ ì‚¬ë„ íŒë‹¨ê³¼ ëª¨ë¸ ì˜ˆì¸¡ ë¹„êµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecmAu5HwgtYa"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.losses import MultipleNegativesRankingLoss, MatryoshkaLoss\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from sentence_transformers import SentenceTransformerTrainer\n",
        "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
        "\n",
        "# ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •\n",
        "# 1. ê¸°ë³¸ ì†ì‹¤: Multiple Negatives Ranking Loss\n",
        "base_loss = MultipleNegativesRankingLoss(emb_model)\n",
        "\n",
        "# 2. Matryoshka Lossë¡œ ë˜í•‘ (ë‹¤ì–‘í•œ ì°¨ì› ì§€ì›)\n",
        "train_loss = MatryoshkaLoss(\n",
        "    emb_model,\n",
        "    base_loss,\n",
        "    matryoshka_dims=[1024, 768, 512, 256, 128, 64]  # ì§€ì›í•  ì°¨ì›ë“¤\n",
        ")\n",
        "\n",
        "# í‰ê°€ì ì„¤ì •\n",
        "evaluator = EmbeddingSimilarityEvaluator(\n",
        "    sentences1=embedding_dataset[\"test\"][\"sentence1\"],\n",
        "    sentences2=embedding_dataset[\"test\"][\"sentence2\"],\n",
        "    scores=embedding_dataset[\"test\"][\"label\"],\n",
        "    name=\"etf-eval\"\n",
        ")\n",
        "\n",
        "# í•™ìŠµ ì „ ì„±ëŠ¥ í‰ê°€\n",
        "score_before = evaluator(emb_model)\n",
        "print(score_before)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgxmFucKgtYa"
      },
      "source": [
        "### ì„ë² ë”© ëª¨ë¸ í•™ìŠµ\n",
        "\n",
        "- LLMë³´ë‹¤ ë¹ ë¦„ (ì‘ì€ ëª¨ë¸)\n",
        "- ë°°ì¹˜ í¬ê¸°ë¥¼ í¬ê²Œ ì„¤ì • ê°€ëŠ¥\n",
        "- ëŒ€ì¡° í•™ìŠµì—ì„œëŠ” í° ë°°ì¹˜ê°€ ìœ ë¦¬\n",
        "- í•™ìŠµë¥ : 2e-5 ê¶Œì¥ (LLMë³´ë‹¤ ë‚®ìŒ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS7eNjW2gtYa"
      },
      "outputs": [],
      "source": [
        "# ì„ë² ë”© í•™ìŠµ\n",
        "emb_trainer = SentenceTransformerTrainer(\n",
        "    model=emb_model,\n",
        "    train_dataset=embedding_dataset['train'],\n",
        "    eval_dataset=embedding_dataset['test'],\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator,\n",
        "    args=SentenceTransformerTrainingArguments(\n",
        "        output_dir=f\"{MODEL_DIR}/etf_embedding\",\n",
        "\n",
        "        # í•™ìŠµ ì„¤ì •\n",
        "        num_train_epochs=1,  # ì´ ì—í¬í¬\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "\n",
        "        # ìµœì í™”\n",
        "        warmup_ratio=0.1,        # ì „ì²´ì˜ 10%ë¥¼ ì›Œë°ì—…\n",
        "        learning_rate=2e-5,      # ì„ë² ë”© ëª¨ë¸ì€ ë‚®ì€ í•™ìŠµë¥ \n",
        "        fp16=True,               # Mixed Precision\n",
        "\n",
        "        # í‰ê°€ ë° ì €ì¥\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=3,            # ìµœê·¼ 3ê°œ ì²´í¬í¬ì¸íŠ¸ë§Œ ìœ ì§€\n",
        "        load_best_model_at_end=True,   # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ\n",
        "\n",
        "        # ë¡œê¹…\n",
        "        logging_steps=10,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"ğŸš€ ì„ë² ë”© í•™ìŠµ ì‹œì‘...\")\n",
        "start_time = time.time()\n",
        "\n",
        "emb_trainer.train()\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"âœ… ì„ë² ë”© í•™ìŠµ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {elapsed/60:.2f}ë¶„)\")\n",
        "\n",
        "# í•™ìŠµ í›„ ì„±ëŠ¥ í‰ê°€\n",
        "score_after = evaluator(emb_model)\n",
        "print(score_after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uS6WmJTgtYa"
      },
      "source": [
        "### ì„ë² ë”© ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1l6vOWCKgtYa"
      },
      "outputs": [],
      "source": [
        "# ì„ë² ë”© ëª¨ë¸ ì €ì¥\n",
        "emb_save_dir = f\"{MODEL_DIR}/etf_embedding_final\"\n",
        "emb_model.save(emb_save_dir)\n",
        "print(f\"âœ… ë¡œì»¬ ì €ì¥: {emb_save_dir}\")\n",
        "\n",
        "# Hugging Face Hubì— ì—…ë¡œë“œ\n",
        "emb_repo = f\"{username}/etf-embedding-bge-m3\"\n",
        "emb_model.push_to_hub(\n",
        "    emb_repo,\n",
        "    token=os.environ[\"HUGGINGFACE_TOKEN\"],\n",
        "    private=True\n",
        ")\n",
        "print(f\"âœ… HF Hub ì—…ë¡œë“œ: https://huggingface.co/{emb_repo}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw4an_o1gtYa"
      },
      "source": [
        "---\n",
        "\n",
        "## 5. ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
        "\n",
        "- **ì½”ì‚¬ì¸ ìœ ì‚¬ë„(Cosine Similarity)**:\n",
        "    - ë‘ ë²¡í„° ì‚¬ì´ì˜ ê°ë„ ì¸¡ì •\n",
        "    - -1 ~ 1 ë²”ìœ„\n",
        "    - 1: ì™„ì „íˆ ë™ì¼í•œ ë°©í–¥\n",
        "    - 0: ì§êµ (ê´€ë ¨ ì—†ìŒ)\n",
        "    - -1: ì™„ì „íˆ ë°˜ëŒ€ ë°©í–¥\n",
        "\n",
        "- **ì„ë² ë”©ì—ì„œì˜ í™œìš©**:\n",
        "    - ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ ì¸¡ì •\n",
        "    - ê²€ìƒ‰: ì¿¼ë¦¬ì™€ ë¬¸ì„œ ìœ ì‚¬ë„\n",
        "    - ì¶”ì²œ: ì•„ì´í…œ ê°„ ìœ ì‚¬ë„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDLReI2GgtYa"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.util import cos_sim\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤\n",
        "test_sentences = [\n",
        "    \"TIGER 200ì€ KOSPI 200ì„ ì¶”ì¢…í•˜ëŠ” ETFì…ë‹ˆë‹¤.\",\n",
        "    \"ì´ ETFëŠ” í•œêµ­ ì£¼ìš” 200ê°œ ê¸°ì—…ì— íˆ¬ìí•©ë‹ˆë‹¤.\",\n",
        "    \"ë ˆë²„ë¦¬ì§€ ETFëŠ” ê³ ìœ„í—˜ ê³ ìˆ˜ìµ ìƒí’ˆì…ë‹ˆë‹¤.\",\n",
        "    \"ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì°¸ ì¢‹ë„¤ìš”.\"\n",
        "]\n",
        "\n",
        "# ì„ë² ë”© ìƒì„±\n",
        "embeddings = emb_model.encode(test_sentences, convert_to_tensor=True)\n",
        "\n",
        "# ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°\n",
        "similarity_matrix = cos_sim(embeddings, embeddings)\n",
        "\n",
        "print(\"\\nğŸ“Š ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ í–‰ë ¬:\\n\")\n",
        "print(\"\\t\" + \"\\t\".join([f\"ë¬¸ì¥{i+1}\" for i in range(len(test_sentences))]))\n",
        "for i, row in enumerate(similarity_matrix):\n",
        "    print(f\"ë¬¸ì¥{i+1}\\t\" + \"\\t\".join([f\"{val:.3f}\" for val in row]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zi1cPqp5ctuW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}